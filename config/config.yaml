# config/config.yaml

# Configuration for the LLM server provided by llama.cpp
llm_server:
  url: "http://localhost:8084/completion"
  headers:
    Content-Type: "application/json"

# IDENTIQUE À main.py
llm_server_vision:
  url: "http://localhost:8088/v1/chat/completions"

# Paths to all required models, relative to the project root directory.
models:
  stt_vosk:
    en: "models/stt_vosk/vosk-model-small-en-us-0.15"
    fr: "models/stt_vosk/vosk-model-small-fr-0.22"
  tts_piper:
    fr_upmc: "models/tts_piper/fr_FR-upmc-medium.onnx"
  llm:
    lfm_8b_q4: "models/llm/LFM2-8B-A1B-Q4_K_M.gguf"
    LFM2-VL-450M-Q4: "models/llm/LFM2-VL-450M-Q4_0.gguf"
    mmproj-LFM2-VL-450M-Q8: "models/llm/mmproj-LFM2-VL-450M-Q8_0.gguf"

# Executable paths and arguments for background services
executables:
  llama_server:
    path: "models/llama_cpp/llama-server.exe"
    args: "-m {model_path} --port 8084 --ctx-size 2048"
  
  # IDENTIQUE À text.txt (sans ctx-size superflu)
  llama_server_vision:
    path: "models/llama_cpp/llama-server.exe"
    args: "-m {model_path} --mmproj {model_path_mmproj} --port 8088"

# Configuration for testing purposes
testing:
  output_dir: "tests/output"
  run_integration_tests: true